# -*- coding: utf-8 -*-
"""ConSurf and SASA Processor with PDB Downloader

Automatically generated by Colab.

Original file is located at
    
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
# STEP 1: Install prerequisites and DSSP from source (CMBI)
!apt-get install -y build-essential cmake git libeigen3-dev
!git clone https://github.com/cmbi/dssp.git
# %cd dssp
!mkdir build
# %cd build
!cmake ..
!make
!make install
!which mkdssp  # confirm installed
# %cd /content

# STEP 2: Install Biopython and Pandas
!pip install biopython pandas

!pip install Bio pandas numpy pydssp

import os
import pandas as pd
from Bio.PDB import PDBParser, PDBList, PDBIO, Select
from Bio.PDB.SASA import ShrakeRupley

class SelectChain(Select):
    """
    Inherits from the Bio.PDB.Select class to allow for the selection
    of a single, specific chain from a PDB structure.
    """
    def __init__(self, chain_id):
        self.chain_id = chain_id

    def accept_chain(self, chain):
        """
        Accepts a chain if its ID matches the target chain ID.

        Args:
            chain (Bio.PDB.Chain.Chain): The chain object to check.

        Returns:
            bool: True if the chain should be included, False otherwise.
        """
        return chain.id == self.chain_id

def find_csv_header_row(file_path):
    """
    Finds the line number of the header row in the ConSurf CSV file.
    The header row is expected to start with 'pos'.

    Args:
        file_path (str): The path to the CSV file.

    Returns:
        int: The line number (0-indexed) of the header row, or None if not found.
    """
    try:
        with open(file_path, 'r') as f:
            for i, line in enumerate(f):
                if line.strip().startswith('pos,SEQ'):
                    return i
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
    except Exception as e:
        print(f"An error occurred while reading {file_path}: {e}")
    return None

def calculate_sasa_and_extract_chain(pdb_id, chain_id, pdb_dir):
    """
    Downloads a PDB file, saves a new file with only the specified chain,
    and calculates SASA for that chain within the context of the full model.

    Args:
        pdb_id (str): The 4-character PDB ID (e.g., '5EQG').
        chain_id (str): The single-character chain ID (e.g., 'A').
        pdb_dir (str): The directory to store PDB files.

    Returns:
        dict: A dictionary mapping residue position to its SASA value.
              Returns an empty dictionary on failure.
    """
    sasa_scores = {}

    try:
        # Step 1: Download the full PDB file. This is necessary for accurate SASA.
        pdbl = PDBList()
        full_pdb_path = pdbl.retrieve_pdb_file(pdb_id, pdir=pdb_dir, file_format='pdb', overwrite=False)

        if not os.path.exists(full_pdb_path):
             print(f"Warning: Failed to download PDB file for {pdb_id}. Skipping.")
             return sasa_scores

        # Step 2: Parse the entire structure from the downloaded file.
        parser = PDBParser(QUIET=True)
        structure = parser.get_structure(pdb_id, full_pdb_path)
        model = structure[0]

        if chain_id not in [c.id for c in model]:
            print(f"Warning: Chain '{chain_id}' not found in {pdb_id}. Skipping.")
            return {}

        # Step 3: Save a NEW PDB file containing ONLY the desired chain.
        io = PDBIO()
        io.set_structure(structure)
        chain_only_filename = os.path.join(pdb_dir, f"{pdb_id}_{chain_id}.pdb")
        io.save(chain_only_filename, SelectChain(chain_id))
        print(f"Successfully saved chain-only PDB: {chain_only_filename}")

        # Step 4: Calculate SASA on the full model for accuracy.
        # The accessibility of a residue depends on its neighbors, even across chains.
        sr = ShrakeRupley()
        sr.compute(model, level="R")  # "R" for residue-level calculation

        # Step 5: Extract SASA scores for the residues in the target chain.
        target_chain = model[chain_id]
        for residue in target_chain:
            res_id = residue.get_id()
            if res_id[0] == ' ':  # Standard residue
                pos = res_id[1]
                sasa_scores[pos] = round(residue.sasa, 2)

    except Exception as e:
        print(f"An error occurred while processing PDB {pdb_id}, chain {chain_id}: {e}")

    return sasa_scores

def process_consurf_files(consurf_dir, pdb_dir, output_dir):
    """
    Main function to process ConSurf files, calculate SASA, and merge the results.
    """
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(pdb_dir, exist_ok=True)
    print(f"Output will be saved in: {output_dir}")
    print(f"PDB files (full and chain-only) will be stored in: {pdb_dir}")

    for filename in os.listdir(consurf_dir):
        if filename.endswith("_msa_positional_aa_frequency.csv"):
            file_prefix = filename.split("_")[0]
            if len(file_prefix) < 5:
                print(f"Warning: Filename '{filename}' invalid format. Skipping.")
                continue

            pdb_id = file_prefix[:4].upper()
            chain_id = file_prefix[4].upper()
            consurf_file_path = os.path.join(consurf_dir, filename)

            print(f"\nProcessing PDB ID: {pdb_id}, Chain: {chain_id}...")

            # Calculate SASA and extract the chain to a new file
            sasa_data = calculate_sasa_and_extract_chain(pdb_id, chain_id, pdb_dir)
            if not sasa_data:
                print(f"Continuing with ConSurf scores for {pdb_id}-{chain_id} without SASA data.")

            # Read ConSurf CSV, skipping metadata rows
            header_row_index = find_csv_header_row(consurf_file_path)
            if header_row_index is None:
                print(f"Warning: Could not find header in {filename}. Skipping.")
                continue

            try:
                consurf_df = pd.read_csv(consurf_file_path, skiprows=header_row_index)
                consurf_df = consurf_df[['pos', 'SEQ', 'SCORE']]
                consurf_df.rename(columns={'pos': 'Position', 'SCORE': 'ConSurf_Grade'}, inplace=True)
                consurf_df['ConSurf_Grade'] = consurf_df['ConSurf_Grade'].astype(str).str.replace('*', '', regex=False)
                consurf_df['ConSurf_Grade'] = pd.to_numeric(consurf_df['ConSurf_Grade'], errors='coerce')
            except Exception as e:
                print(f"Error reading CSV file {filename}: {e}")
                continue

            # Merge ConSurf grades with SASA scores
            if sasa_data:
                sasa_df = pd.DataFrame(list(sasa_data.items()), columns=['Position', 'SASA_Score'])
                merged_df = pd.merge(consurf_df, sasa_df, on='Position', how='left')
                merged_df['SASA_Score'].fillna('N/A', inplace=True)
            else:
                merged_df = consurf_df
                merged_df['SASA_Score'] = 'N/A'

            # Write the merged data to a new file
            output_filename = os.path.join(output_dir, f"{pdb_id}{chain_id}_consurf_sasa_grades.txt")
            merged_df.to_csv(output_filename, sep='\t', index=False)
            print(f"Successfully created output file: {output_filename}")


if __name__ == "__main__":
    # --- Configuration ---
    # !! IMPORTANT: Update these paths to match your Google Drive structure !!
    CONSURF_CSV_DIR = '/content/drive/MyDrive/Transmembrane evolution/Consurf/Consurf_csv'
    PDB_FILES_DIR = '/content/drive/MyDrive/Transmembrane evolution/Consurf/pdb_files'
    OUTPUT_DIR = '/content/drive/MyDrive/Transmembrane evolution/consurf_grades_with_sasa'

    # --- Run the processor ---
    process_consurf_files(CONSURF_CSV_DIR, PDB_FILES_DIR, OUTPUT_DIR)
    print("\nAll files processed.")

!apt install dssp

!pip install biopython freesasa

import os
import freesasa
import pandas as pd
from Bio import PDB

# Folder where your files are
folder = "/content/drive/MyDrive/Transmembrane evolution/Consurf/pdb_files"

# Create mapping of ent files → list of chain IDs
# e.g. { "pdb7w9k.ent" : ["A", "B"] }
mapping = {}

# First find all pdb files
for fname in os.listdir(folder):
    if fname.endswith(".pdb"):
        pdbid_chain = fname.split(".")[0]     # e.g. "7W9K_A"
        pdbid, chain = pdbid_chain.split("_")
        ent_file = f"pdb{pdbid.lower()}.ent"
        if ent_file not in mapping:
            mapping[ent_file] = []
        mapping[ent_file].append(chain)

print("Mapping found:")
print(mapping)

# Prepare results list
results = []

# Biopython parsers
parser = PDB.PDBParser(QUIET=True)
io = PDB.PDBIO()

for ent_file, chain_list in mapping.items():
    ent_path = os.path.join(folder, ent_file)

    if not os.path.exists(ent_path):
        print(f"File missing: {ent_file}")
        continue

    # Load ENT structure
    structure = parser.get_structure(ent_file, ent_path)

    model = structure[0]

    for chain_id in chain_list:
        if chain_id not in model:
            print(f"Chain {chain_id} not found in {ent_file}")
            continue

        chain = model[chain_id]

        # Save chain as temporary PDB file
        tmp_filename = os.path.join(folder, f"tmp_{ent_file}")
        io.set_structure(chain)
        io.save(tmp_filename)

        # Run freesasa
        fs_struct = freesasa.Structure(tmp_filename)
        result = freesasa.calc(fs_struct)
        sasa = result.totalArea()

        results.append({
            "ent_file": ent_file,
            "chain_id": chain_id,
            "sasa": sasa
        })

        os.remove(tmp_filename)

# Save results
df = pd.DataFrame(results)
csv_path = os.path.join(folder, "sasa_results.csv")
df.to_csv(csv_path, index=False)

print(f"Saved results to: {csv_path}")

import os
import freesasa
import pandas as pd
from Bio import PDB

# Folder where files live
folder = "/content/drive/MyDrive/Transmembrane evolution/Consurf/pdb_files"
csvfolder = "/content/drive/MyDrive/Transmembrane evolution/Consurf"

# Build mapping of ent files → list of chains
mapping = {}

for fname in os.listdir(folder):
    if fname.endswith(".pdb"):
        pdbid_chain = fname.split(".")[0]   # e.g. "7W9K_A"
        pdbid, chain = pdbid_chain.split("_")
        ent_file = f"pdb{pdbid.lower()}.ent"
        if ent_file not in mapping:
            mapping[ent_file] = []
        mapping[ent_file].append(chain)

print("Mapping found:")
print(mapping)

# Results list
results = []

# Biopython objects
parser = PDB.PDBParser(QUIET=True)
io = PDB.PDBIO()

for ent_file, chain_list in mapping.items():
    ent_path = os.path.join(folder, ent_file)

    if not os.path.exists(ent_path):
        print(f"File missing: {ent_file}")
        continue

    # Parse ENT file
    structure = parser.get_structure(ent_file, ent_path)
    model = structure[0]

    for chain_id in chain_list:
        if chain_id not in model:
            print(f"Chain {chain_id} not found in {ent_file}")
            continue

        chain = model[chain_id]

        # Save only this chain to temp file
        tmp_filename = os.path.join(folder, f"tmp_{ent_file}_{chain_id}.pdb") # Added chain_id to tmp filename
        io.set_structure(chain)
        io.save(tmp_filename)

        # Run freesasa
        fs_struct = freesasa.Structure(tmp_filename)
        result = freesasa.calc(fs_struct)

        # Get per-residue results
        areas = result.residueAreas()

        # areas is a dictionary where keys are chain IDs (single char)
        # and values are dictionaries of residue areas
        for current_chain_id, residue_areas in areas.items():
            # residue_areas is a dictionary like:
            # { '1' : <freesasa.ResidueArea object>,
            #   '2' : <freesasa.ResidueArea object>, ... }
            for resnum_key, values in residue_areas.items():
                 # freesasa's resnum_key can be just the number (string)
                 # or include insertion codes, e.g., '10A'
                try:
                    resnum = int(resnum_key) # Convert resnum to integer
                except ValueError:
                    # Handle cases with insertion codes if necessary,
                    # for simplicity we might skip these or use a different approach
                    print(f"Skipping residue with non-integer number: {resnum_key} in chain {current_chain_id} of {ent_file}")
                    continue

                # To get the residue name, we need to access the Biopython structure again
                try:
                    # Attempt to find the residue by number (and potentially insertion code if needed)
                    # This part might need refinement depending on how insertion codes are handled by freesasa and Biopython
                    residue = chain[(' ', resnum, ' ')] # Common case for standard residues
                    resname = residue.get_resname()
                except KeyError:
                     print(f"Warning: Residue {resnum_key} not found in Biopython structure for chain {current_chain_id} of {ent_file}. Skipping.")
                     resname = "Unknown"


                # Access the total SASA from the ResidueArea object
                sasa = values.total


                results.append({
                    "ent_file": ent_file,
                    "chain_id": current_chain_id,
                    "residue_number": resnum,
                    "residue_name": resname,
                    "sasa": sasa
                })

        os.remove(tmp_filename)

# Save all residues to CSV
df = pd.DataFrame(results)
csv_path = os.path.join(csvfolder, "sasa_per_residue.csv")
df.to_csv(csv_path, index=False)

print(f"Saved per-residue SASA results to:\n{csv_path}")

import os
import freesasa
import pandas as pd
from Bio import PDB
from Bio.PDB.SASA import ShrakeRupley

# Folder where files live
folder = "/content/drive/MyDrive/Transmembrane evolution/Consurf/pdb_files"
csvfolder = "/content/drive/MyDrive/Transmembrane evolution/Consurf"


# Max ASA values from Tien et al. 2013
max_sasa_dict = {
    'ALA': 121.0,
    'ARG': 265.0,
    'ASN': 187.0,
    'ASP': 187.0,
    'CYS': 148.0,
    'GLN': 214.0,
    'GLU': 214.0,
    'GLY': 97.0,
    'HIS': 216.0,
    'ILE': 195.0,
    'LEU': 191.0,
    'LYS': 230.0,
    'MET': 203.0,
    'PHE': 228.0,
    'PRO': 154.0,
    'SER': 143.0,
    'THR': 163.0,
    'TRP': 264.0,
    'TYR': 255.0,
    'VAL': 165.0
}

# Build mapping: ent file → chain IDs
mapping = {}
for fname in os.listdir(folder):
    if fname.endswith(".pdb"):
        name = fname.split(".")[0]
        pdbid, chain = name.split("_")
        ent_file = f"pdb{pdbid.lower()}.ent"
        mapping.setdefault(ent_file, []).append(chain)

print("Mapping found:", mapping)

# Biopython objects
parser = PDB.PDBParser(QUIET=True)
# io = PDB.PDBIO() # No longer needed

results = []

for ent_file, chain_list in mapping.items():
    ent_path = os.path.join(folder, ent_file)
    if not os.path.exists(ent_path):
        print(f"Missing file: {ent_file}")
        continue

    structure = parser.get_structure(ent_file, ent_path)
    model = structure[0] # Assuming only one model

    # Calculate SASA for the entire model
    sr = ShrakeRupley()
    sr.compute(model, level="R") # Calculate SASA at the residue level

    # Iterate through residues in the model and collect data for target chains
    for chain in model:
        if chain.id in chain_list:
            for residue in chain:
                res_id = residue.get_id()
                # Filter out hetero-atoms and water
                if res_id[0] == ' ':
                    res_num = res_id[1]
                    res_name = residue.get_resname()
                    sasa = round(residue.sasa, 2) # Get the calculated SASA

                    # Calculate RSA
                    max_sasa = max_sasa_dict.get(res_name.upper(), None)
                    if max_sasa:
                        rsa = (sasa / max_sasa) * 100
                    else:
                        rsa = None # Or handle as appropriate, e.g., 0 or 'N/A'

                    results.append({
                        "ent_file": ent_file,
                        "chain_id": chain.id,
                        "residue_number": res_num,
                        "residue_name": res_name,
                        "sasa": sasa,
                        "rsa": rsa
                    })


# Save to CSV
df = pd.DataFrame(results)
csv_path = os.path.join(csvfolder, "residue_sasa_results_with_rsa.csv")
df.to_csv(csv_path, index=False)
print(f"Saved CSV with RSA to: {csv_path}")

import pandas as pd
import os

# === Setup paths ===
sasa_path = "/content/drive/MyDrive/Transmembrane evolution/Consurf/residue_sasa_results_with_rsa.csv"
consurf_folder = "/content/drive/MyDrive/Transmembrane evolution/Consurf/Consurf_csv"

# === Load SASA CSV ===
df_sasa = pd.read_csv(sasa_path)

# Create empty column for consurf
df_sasa["consurf_grade"] = None

# === Build dictionary: ent_file → consurf DataFrame ===
consurf_data = {}

for fname in os.listdir(consurf_folder):
    if not fname.endswith(".csv"):
        continue

    pdbid = fname.split("_")[0].lower()   # e.g. 2ZW3E → 2zw3e
    ent_file = f"pdb{pdbid}.ent"

    fpath = os.path.join(consurf_folder, fname)

    try:
        # Read skipping 3 header rows
        df = pd.read_csv(fpath, skiprows=3, delimiter="\t|,", engine="python")

        if "pos" not in df.columns or "ConSurf Grade" not in df.columns:
            print(f"Missing expected columns in {fname}")
            continue

        # Store with lowercase keys
        consurf_data[ent_file] = df[["pos", "ConSurf Grade"]].copy()
        consurf_data[ent_file].columns = ["residue_number", "consurf_grade"]
    except Exception as e:
        print(f"Error reading {fname}: {e}")
        continue

# === Merge each row in SASA with corresponding consurf grade ===
def get_consurf(ent_file, resnum):
    try:
        df = consurf_data.get(ent_file.lower())
        if df is None:
            return None
        row = df[df["residue_number"] == resnum]
        if not row.empty:
            return row.iloc[0]["consurf_grade"]
        return None
    except:
        return None

df_sasa["consurf_grade"] = df_sasa.apply(
    lambda row: get_consurf(row["ent_file"], row["residue_number"]),
    axis=1
)

# === Save new merged CSV ===
output_path = "/content/drive/MyDrive/Transmembrane evolution/Consurf/residue_sasa_with_consurf.csv"
df_sasa.to_csv(output_path, index=False)
print(f"Merged CSV with ConSurf saved to: {output_path}")

import os
import pandas as pd

# Folders
sasa_path = "/content/drive/MyDrive/Transmembrane evolution/Consurf/residue_sasa_results_with_rsa.csv"
consurf_folder = "/content/drive/MyDrive/Transmembrane evolution/Consurf/Consurf_csv"

# Load SASA CSV
sasa_df = pd.read_csv(sasa_path)

# To store final merged rows
merged_rows = []

# Group SASA data by ent_file and chain_id
grouped = sasa_df.groupby(['ent_file', 'chain_id'])

for (ent_file, chain_id), group_df in grouped:
    # Build ConSurf filename
    ent_stem = ent_file.replace("pdb", "").replace(".ent", "").upper()
    consurf_file = f"{ent_stem}{chain_id}_msa_positional_aa_frequency.csv"
    consurf_path = os.path.join(consurf_folder, consurf_file)

    if not os.path.exists(consurf_path):
        print(f"ConSurf file missing: {consurf_file}")
        # Optionally, add NaN grade for all residues instead
        group_df["consurf_grade"] = None
        merged_rows.append(group_df)
        continue

    # Load ConSurf file
    # Find the header row (since your example has text above)
    skip_rows = 0
    with open(consurf_path) as f:
        for i, line in enumerate(f):
            if line.startswith("pos"):
                skip_rows = i
                break

    consurf_df = pd.read_csv(consurf_path, skiprows=skip_rows)

    # Keep only pos and ConSurf Grade
    consurf_df = consurf_df[["pos", "ConSurf Grade"]]

    # Rename for merge
    consurf_df.rename(columns={"pos": "residue_number"}, inplace=True)

    # Merge on residue number
    merged = pd.merge(
        group_df,
        consurf_df,
        on="residue_number",
        how="left"
    )

    merged_rows.append(merged)

# Combine all groups
final_df = pd.concat(merged_rows, ignore_index=True)

# Save merged CSV
output_path = os.path.join("/content/drive/MyDrive/Transmembrane evolution/Consurf", "sasa_with_consurf.csv")
final_df.to_csv(output_path, index=False)
print(f"Saved merged file to {output_path}")

import pandas as pd
import os

# --- 1. Setup: Create dummy files and folders for demonstration ---
# This part of the code simulates your file environment.
# In your real use case, you can comment this section out and just
# make sure your files are uploaded to Colab correctly.

# Create a directory for the ConSurf files
os.makedirs('consurf_csv', exist_ok=True)

# Create the main SASA data file
sasa_data = """ent_file,chain_id,residue_number,residue_name,sasa,rsa
pdb2zw3.ent,E,2,ASP,53.32,28.51336898
pdb2zw3.ent,E,3,TRP,121.33,45.95833333
pdb2zw3.ent,E,4,GLY,30.51,31.45360825
pdb2zw3.ent,E,5,THR,53.38,32.74846626
pdb2zw3.ent,E,6,LEU,55.4,29.0052356
pdb2zw3.ent,E,7,GLN,57.88,27.04672897
pdb2zw3.ent,E,8,THR,42.91,26.32515337
pdb2zw3.ent,E,9,ILE,117.93,60.47692308
pdb2zw3.ent,E,10,LEU,66.15,34.63350785
pdb2zw3.ent,E,11,GLY,33.51,34.54639175
pdb2zw3.ent,E,12,GLY,41.28,42.55670103
pdb2zw3.ent,E,13,VAL,55.94,33.9030303
pdb2zw3.ent,E,14,ASN,114.17,61.05347594
"""
# Use the new filename for the dummy file creation
with open('residue_sasa_results_with_rsa.csv', 'w') as f:
    f.write(sasa_data)

# Create the corresponding ConSurf data file
# The filename is '2ZW3' (from pdb2zw3.ent) + 'E' (chain_id) + suffix
consurf_data = """The table details the residue variety in % for each position in the query sequence.
Each column shows the % for that amino-acid, found in position ('pos') in the MSA.
In case there are residues which are not a standard amino-acid in the MSA, they are represented under column 'OTHER'


pos,A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V,W,Y,OTHER,MAX AA,ConSurf Grade
1,,,,1.852,,,,,98.148,,,,,,,,,,,M 98.148,9
2,0.87,,37.391,17.391,,,0.87,0.87,,,22.609,,,18.261,1.739,,,,D 37.391,7
3,0.735,,,,2.941,,,,1.471,,0.735,,,,,,93.382,0.735,,,W 93.382,7
4,13.475,,14.184,2.837,,29.787,2.128,,4.965,,0.709,5.674,1.418,,0.709,21.277,2.837,,,,G 29.787,3
5,11.644,,,,40.411,1.37,,3.425,2.055,11.644,,,,,10.959,13.014,2.74,0.685,2.055,,F 40.411,5
6,1.282,,,,9.615,,,1.923,,81.41,0.641,,,,,,5.128,,,,L 81.410,8
7,1.235,,1.235,15.432,0.617,25.926,0.617,,3.704,,,,9.259,9.259,11.111,8.642,0.617,,12.346,,G 25.926,4
8,11.561,1.734,5.78,2.89,6.936,8.671,4.624,1.156,9.249,0.578,0.578,1.734,0.578,6.358,17.919,12.139,5.202,2.312,,,,R 17.919,2
9,0.532,0.532,,,,4.255,,,11.702,,67.553,0.532,,,4.787,,,9.574,0.532,,,L 67.553,6
10,0.518,,,,12.953,,,10.363,,73.575,,,,,2.591,,,,L 73.575,8
11,9.596,2.02,15.657,22.727,0.505,5.051,,8.586,2.02,2.02,0.505,2.02,,1.01,0.505,21.717,1.01,5.051,,,,E 22.727,7
12,,,,,,,,,,,,,,,,,,,,,,
13,,,,,,,,,,,,,,,,,,,,,,
14,,,,,,,,,,,,,,,,,,,,,,
"""
with open('consurf_csv/2ZW3E_msa_positional_aa_frequency.csv', 'w') as f:
    f.write(consurf_data)

print("--- Setup Complete: Dummy files created. ---")
print("\n")


# --- 2. Main Logic: Load data and merge ---

# Define paths
sasa_file_path = '/content/drive/MyDrive/Transmembrane evolution/Consurf/residue_sasa_results_with_rsa.csv' # <-- UPDATED FILENAME
consurf_folder_path = '/content/drive/MyDrive/Transmembrane evolution/Consurf/Consurf_csv/'
output_file_path = '/content/drive/MyDrive/Transmembrane evolution/Consurf/2sasa_with_consurf_grades.csv'


# Load the main SASA dataframe
try:
    sasa_df = pd.read_csv(sasa_file_path)
except FileNotFoundError:
    print(f"Error: The main SASA file was not found at {sasa_file_path}")
    exit() # Exit if the main file is not found

# This list will hold the dataframes for each protein, which we'll combine at the end
all_merged_data = []

# Group by the source file and chain to process each protein structure individually
for (ent, chain), group in sasa_df.groupby(['ent_file', 'chain_id']):
    print(f"Processing: {ent}, Chain: {chain}")

    # Construct the ConSurf filename from the sasa data
    # e.g., 'pdb2zw3.ent' -> '2ZW3'
    pdb_id = ent[3:7].upper()
    consurf_filename = f"{pdb_id}{chain}_msa_positional_aa_frequency.csv"
    consurf_file_path = os.path.join(consurf_folder_path, consurf_filename)

    # Check if the corresponding ConSurf file exists
    if not os.path.exists(consurf_file_path):
        print(f"  -> Warning: ConSurf file not found: {consurf_file_path}. Skipping this group.")
        continue

    # Load the ConSurf data
    try:
        # --- ROBUST HEADER FINDING LOGIC ---
        # Dynamically find the header row to avoid issues with varying numbers of comment lines.
        header_row_index = None
        with open(consurf_file_path, 'r') as f:
            for i, line in enumerate(f):
                # The header row is the first one that starts with 'pos,'
                if line.strip().startswith('pos,'):
                    header_row_index = i
                    break

        if header_row_index is None:
            print(f"  -> Warning: Could not find a header row starting with 'pos,' in {consurf_filename}. Skipping file.")
            continue

        # Load the ConSurf data, skipping all lines before the dynamically found header.
        consurf_df = pd.read_csv(consurf_file_path, skiprows=header_row_index)
        # --- END OF ROBUST LOGIC ---

        # Clean up column names by removing leading/trailing whitespace
        consurf_df.columns = consurf_df.columns.str.strip()

        # Keep only the essential columns: position and grade
        consurf_df = consurf_df[['pos', 'ConSurf Grade']]

        # Rename 'pos' to 'residue_number' to allow for a clean merge
        consurf_df = consurf_df.rename(columns={'pos': 'residue_number'})

        # Merge the sasa data for this group with its corresponding consurf data
        merged_group = pd.merge(
            group,
            consurf_df,
            on='residue_number',
            how='left' # Use 'left' to keep all original sasa rows, even if no grade is found
        )

        all_merged_data.append(merged_group)
        print(f"  -> Successfully merged with {consurf_filename}")

    except KeyError as e:
        print(f"  -> Error: A required column was not found in {consurf_filename} after reading. Please check the file's format. Details: {e}")
    except Exception as e:
        print(f"  -> An unexpected error occurred while processing file {consurf_filename}: {e}")


# --- 3. Finalize and Save ---

if all_merged_data:
    # Concatenate all the processed groups back into a single dataframe
    final_df = pd.concat(all_merged_data, ignore_index=True)

    # Save the final merged dataframe to a new CSV file
    final_df.to_csv(output_file_path, index=False)

    print("\n--- Processing Finished ---")
    print(f"Final merged data saved to: {output_file_path}")
    print("\nPreview of the final data:")
    print(final_df.head(20))
else:
    print("\n--- Processing Finished ---")
    print("No data was merged. Please check file paths and formats.")